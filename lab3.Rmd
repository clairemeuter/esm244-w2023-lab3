---
title: 'Lab week 3: binomial logistic regression'
author: "Claire Meuter"
date: "2023-01-26"
output: html_document
---

```{r setup, echo =TRUE, warning=,FALSE, message=FALSE, include=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(tidymodels)
library(palmerpenguins)
library(GGally)
library(jtools)
library(AICcmodavg)
```

# Psuedocode (or a todo list)
# think the steps through 
* Examine our data
* Identify a question
* Wrangle the data 
* Identify some candidate models 
* Select among candidate models using AIC/BIC
* Select among candidate models using K-fold cross validation
* Select amon candidate models using area under reciever operating characteristic curve 


#explore our data a little
```{r}
GGally::ggpairs(penguins %>% select(species, bill_length_mm:sex), aes(color = species))
```

```{r}
class(penguins$species)
#it's a factor
levels(penguins$species)

adelie_chinstrap <- penguins %>% 
  filter(species %in% c("Adelie", "Chinstrap")) %>% 
  mutate(species = fct_drop(species)) %>% #telling it to drop any factors that don't exist
  select(-year) %>% 
  drop_na()

levels(adelie_chinstrap$species) #now my levels only contain adelie and chinstrap 


```

#let's check out trends across variables 
```{r}

ggplot(data = adelie_chinstrap, aes(x = body_mass_g, y = flipper_length_mm)) +
  geom_point(aes(color = sex, shape = island)) +
  facet_wrap(~species)

ggplot(data = adelie_chinstrap, aes(x= body_mass_g,  y = bill_length_mm)) +
  geom_point(aes(color = sex, shape = island)) +
  facet_wrap(~species)

```

## Let's do some binary logistical regression 
```{r}
#let's create a formula 
f1 <- species ~ body_mass_g + flipper_length_mm + sex

#Let's first try to predict penguin species as a function of body mass, flipper length, and sex
ad_chin_blr1 <- glm(formula= f1, data = adelie_chinstrap, 
                    family = "binomial")
summary(ad_chin_blr1)

```
```{r}
# Get a tidy version w/ broom:
blr1_tidy <- broom::tidy(ad_chin_blr1)
```

Does this align with the mass comparisons for Chinstraps & Adelies we see?
```{r}
ggplot(data = adelie_chinstrap, aes(x = species, y = flipper_length_mm)) +
  geom_jitter(aes(color = sex))
```


```{r}
blr1_fitted <- ad_chin_blr1 %>%
  broom::augment(type.predict = "response")

ggplot(data = blr1_fitted, aes(x = flipper_length_mm, y = .fitted)) +
  geom_point(aes(color=sex, shape = species)) +
  geom_smooth(aes(color = sex), se = FALSE) +
  labs(x = "Flipper length (mm)",
   	   y = "Probability of outcome Chinstrap")
  
```


#prediction for new values with predict ()
```{r}
ex1 <- predict(ad_chin_blr1, 
               data_frame(sex = "female",
                           body_mass_g = 3410,
                  flipper_length_mm = 192),
                # tell it type = 'response' to get prob, not log odds
                type = "response")
ex1
#ex1 = .402 so there's a 40% chance that it's a chinstrap

new_df <- data.frame(
  sex = c("male", "male", "female"),
  body_mass_g = c(3298, 4100, 3600),
  flipper_length_mm = c(212, 175, 180)
)


ex2 <- predict(ad_chin_blr1, new_df, type = "response")
#93% chance first is chinstrap
#1% and 6% chance for the other two that their chinstrap 
```

##create a new binary logistic model 
```{r}
f2 <-species ~ bill_length_mm + body_mass_g

ad_chin_blr2 <- glm(formula = f2, data = adelie_chinstrap, family = "binomial")

ad_chin_blr2
summary(ad_chin_blr2)

# Get a tidy version w/ broom:
blr2_tidy <- broom::tidy(ad_chin_blr2)
```

Let's see if this makes sense based on a visual comparison:
```{r}
ggplot(adelie_chinstrap, aes(x = bill_length_mm, y = body_mass_g)) +
  geom_point(aes(color = species))
```

## Model selection 
```{r}
AICcmodavg::aictab(list(ad_chin_blr1, ad_chin_blr2))
AICcmodavg::bictab(list(ad_chin_blr1, ad_chin_blr2))
```
10-fold cross validation 
```{r}
set.seed(123)


n_folds <- 10
fold_vec <- rep(1:n_folds, length.out = nrow(adelie_chinstrap))

ad_chin_kfold <- adelie_chinstrap %>%
  mutate(fold = sample(fold_vec, size = n(), replace = FALSE))


```

purrr::map()
```{r}
x_vec <- 1:10
thing <- purrr::map(.x = x_vec, # a squenece (vector, list) 
                    .f = sqrt) # a function basically works similar to a for loop 
thing

my_funct <- function(x, y, z) {
  return((x - y) ^ z)
}

thing2 <- purrr::map(.x = x_vec,      # a sequence (for first arg of function)
                     .f = my_funct,   # name of a function to apply
                     y = 2, z = 3)    # additional parameters (for other args)
thing2

```

```{r}
# function to calculate accuracy, given a "truth" vector and "prediction" vector
pred_acc <- function(x, y) {
  accurate <- ifelse(x == y, 1, 0)
  
  return(mean(accurate, na.rm = TRUE))
}



# function to calculate accuracy of BLR of one fold (training and testing)
calc_fold <- function(i, fold_df, f) {
  kfold_test <- fold_df %>%
    filter(fold == i)
  kfold_train <- fold_df %>%
    filter(fold != i)
  
  kfold_blr <- glm(f, data = kfold_train, family = 'binomial')
  kfold_pred <- kfold_test %>%
    mutate(blr = predict(kfold_blr, kfold_test, type = 'response')) %>%
    mutate(pred = ifelse(blr > 0.50, 'Chinstrap', 'Adelie'))
  
  kfold_accuracy <- kfold_pred %>%
    summarize(blr_acc = pred_acc(species, pred)) # using my other function
  
  return(kfold_accuracy)
}

```

```{r}
results1_purrr_df <- purrr::map(.x = 1:n_folds, # sequence of fold numbers
                                .f = calc_fold, # function
                                fold_df = ad_chin_kfold, # additional argument to calc_fold()
                                f = f1) %>%              # additional argument to calc_fold()
  bind_rows() %>%
  mutate(mdl = 'f1')

results2_purrr_df <- purrr::map(.x = 1:n_folds, .f = calc_fold, 
                               fold_df = ad_chin_kfold,
                               f = f2) %>%
  bind_rows() %>%
  mutate(mdl = 'f2')

results_purr_df <-bind_rows(results1_purrr_df, results2_purrr_df) %>% 
   group_by(mdl) %>%
  summarize(mean_acc = mean(blr_acc))
```
#tidymodels version 
```{r}
## define model type 
blr_model <- logistic_reg() %>% 
  set_engine("glm")  #beauty of tiny models is I don't need to know a lot about these models, tiny models takes care of that when I set the engines 

### set a basic regression 
blr_tidyfit_f1 <- blr_model %>% 
  fit(f1, data = adelie_chinstrap)

blr_tidyfit_f2 <- blr_model %>% 
  fit(f2, data = adelie_chinstrap)

blr_tidyfit_f1
blr_tidyfit_f2

blr_tidyfit_f1 %>% 
  tidy()

blr_tidyfit_f2 %>% 
  glance()
```
#tidy kfold cross validation 
```{r}
set.seed(345)

tidy_folds <- vfold_cv(adelie_chinstrap, v = 10)
tidy_folds

### use a workflow to bundle a model and formula 
blr_tidy_wf1 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f1)

blr_tidy_cv_f1 <- blr_tidy_wf1 %>%
  fit_resamples(tidy_folds)

blr_tidy_cv_f1

collect_metrics(blr_tidy_cv_f1)

#looking at sample 2
blr_tidy_wf2 <- workflow() %>%
  add_model(blr_model) %>%
  add_formula(f2)

blr_tidy_cv_f2 <- blr_tidy_wf2 %>%
  fit_resamples(tidy_folds)

blr_tidy_cv_f2

collect_metrics(blr_tidy_cv_f2)
```
## Area under the curve!

Receiver Operating Characteristic Curve (ROC Curve) compares the diagnostic ability of a binary classifier (like logistic regression) based on the discrimination threshold.  Up to now (and for homework) we've been using a 50% threshold by default.  The ROC can tell us tradeoffs between true positive rate and false positive rate as we change the threshold, and also can give a great indication of model quality.

It seems like model 2 is far better than model 1 in this instance.
```{r}
blr_f1_pred <- adelie_chinstrap %>%
  mutate(predict(blr_tidyfit_f1, .),
         predict(blr_tidyfit_f1, ., type = 'prob'))

blr_f1_pred %>%
  roc_curve(truth = species, .pred_Adelie) %>%
  autoplot()
```

